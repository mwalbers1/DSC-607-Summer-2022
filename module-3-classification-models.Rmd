---
title: "DSC 607 Data Mining: Classification using kNN and Decision Trees"

date: "`r Sys.Date()`"

output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

1. Create Decision Tree and KNN classifiers on a data set without cross validation. 

2. Then using cross-validation create classification decision tree and KNN models on same data set.  


Compare results between first two cases:

- Which classifier was more accurate?

- Which strategy performed best? cross-validation or without cross-validation?

- What statistic was used to compare performance?


## Data Set

This is an Olympic Games dataset that describes medals and athletes for Tokyo 2020. The data was created from the [2020 Tokyo Olympics](https://olympics.com/tokyo-2020/en/). More than 2,400 medals, and 11,000 athletes (with some personal data: date and place of birth, height, etc.) of the XXXII Olympic Games is in the data set.  

The medals.csv file was downloaded from [Kaggle](https://www.kaggle.com/datasets/piterfm/tokyo-2020-olympics?select=medals.csv)


**Author of Data Set:**

Petro

Kaggle Expert

Lviv, Lviv Oblast, Ukraine

Data Scientist, PhD


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(skimr)
library(janitor)
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(class)
library(caret)
library(rpart)
```

```{r lessR library, echo=FALSE, warning=FALSE, message=FALSE}
library(lessR)
```

## Read CSV file

```{r}
medals_data <- read.csv('data/medals.csv')
```

```{r}
str(medals_data)
```

## Data Cleaning

```{r}
skim_without_charts(medals_data)
```

```{r}
medals_data_trimmed <- medals_data %>% 
  select(medal_type, athlete_short_name, athlete_sex, country_code, country, discipline_code, event)
```

```{r}
head(medals_data_trimmed)
```
```{r glimpse medals data trimmed}
glimpse(medals_data_trimmed)
```


### Display proportion of medal types


```{r donut chart of medal types}
PieChart(x = medal_type, data = medals_data_trimmed, values="%", main = "Olympic Medals")
```
This donut chart shows an equal proportion of medal types for Gold, Silver and Bronze medals


## Check for Outliers

```{r contigency table discipline_code}
discipline_code_freq <- table(medals_data_trimmed$discipline_code) %>% as.data.frame %>% arrange(Freq)
```

```{r}
distinct_discipline <- medals_data %>% 
  select(c(discipline_code, discipline)) %>% 
  unique()
```

```{r}
discipline_freq <- merge(discipline_code_freq, distinct_discipline, by.x = 'Var1', by.y = 'discipline_code', all.x = TRUE) %>% 
  arrange(Freq)
```

```{r}
discipline_freq <- clean_names(discipline_freq)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
discipline_freq_new <- discipline_freq %>% 
  select(var1, freq, discipline) %>% 
  rename(var1, discipline_code) %>% 
  rename(freq, frequency)
```


```{r histogram for discipline_freq_new}
ggplot(data = discipline_freq_new) +
  geom_histogram(mapping = aes(x=frequency), bins = 50, color="white", fill="purple") +
  scale_x_continuous(breaks = seq(min(discipline_freq_new$frequency), max(discipline_freq_new$frequency), by = 15)) +
  scale_y_continuous(breaks = seq(1, 15, by=2)) +
  labs(title="Discipline Frequency",
       x="Frequency",
       y="Count")
```


There are nine disciplines in which six medals were won. Since this far exceeds the other disciplines, the disciplines with six medals won will be excluded from the analysis. 


```{r}
discipline_outliers <- discipline_freq_new %>% 
  filter(frequency == 6) %>% 
  select(discipline_code)
```


### Create kNN data frame

- Remove outliers  

- Select attributes for kNN model


```{r}
medals_knn <- medals_data_trimmed %>% 
  filter(!(discipline_code %in% discipline_outliers$discipline_code)) %>% 
  select(medal_type, country_code, discipline_code, event)
```

```{r}
str(medals_knn)
```


### Create Decision Tree data frame

```{r}
medals_decision_tree <- medals_data_trimmed %>% 
  filter(!(discipline_code %in% discipline_outliers$discipline_code)) %>% 
  select(medal_type, country_code, discipline_code, event)
```

```{r}
str(medals_decision_tree)
```


## Data Transformation

Preprocess data for the K-Nearest Neighbors and Decision Tree models


### kNN data preparation

Convert the three categorical variables, country_code, discipline_code, and event into factors.  

```{r}
medals_knn$medal_type <- as.factor(medals_knn$medal_type)
medals_knn$country_code <- as.factor(medals_knn$country_code)
medals_knn$discipline_code <- as.factor(medals_knn$discipline_code)
medals_knn$event <- as.factor(medals_knn$event)
```


```{r}
summary(medals_knn)
```


#### Standardize columns for kNN

Standardize country_code, discipline_code, and event columns to have mean of zero and standard deviation of one


```{r}
medals_knn$country_code_int <- as.integer(medals_knn$country_code)
medals_knn$discipline_code_int <- as.integer(medals_knn$discipline_code)
medals_knn$event_int <- as.integer(medals_knn$event)
```

```{r}
medals_knn$country_code_scaled <- scale(medals_knn$country_code_int)
medals_knn$discipline_code_scaled <- scale(medals_knn$discipline_code_int)
medals_knn$event_scaled <- scale(medals_knn$event_int)
```

```{r summary medals_knn after scaling}
summary(medals_knn)
```

```{r}
colnames(medals_knn)
```


### Create binary columns for medal type column for Decision Tree data frame

```{r}
medals_decision_tree$gold <- ifelse(medals_decision_tree$medal_type == "Gold Medal", 1, 0)
medals_decision_tree$silver <- ifelse(medals_decision_tree$medal_type == "Silver Medal", 1, 0)
medals_decision_tree$bronze <- ifelse(medals_decision_tree$medal_type == "Bronze Medal", 1, 0)
```

```{r}
medals_decision_tree$gold <- as.factor(medals_decision_tree$gold)
medals_decision_tree$silver <- as.factor(medals_decision_tree$silver)
medals_decision_tree$bronze <- as.factor(medals_decision_tree$bronze)
```


```{r}
summary(medals_decision_tree)
```


## k-Nearest Neighbors Model

- Create training and test sets from medals_knn data frame where 80% of the data will be for training. The remaining 20% of the data will be for the test set

- Create k-Nearest Neighbors model for classification of medal types (Gold, Silver, and Bronze)


```{r}
shuffle_data <- function(medals.df) {
  # Set seed
  set.seed(42)

  # Shuffle row indices: rows
  rows <- sample(nrow(medals.df))

  # Randomly order data
  medals.df[rows,]
}
```

```{r}
shuffled_medals_knn <- shuffle_data(medals_knn)
```

```{r}
split <- round(nrow(shuffled_medals_knn)*0.80)
```

country_code_scaled    "discipline_code_scaled"
[10] "event_scaled"          

```{r}
medals_knn_train <- shuffled_medals_knn[1:split, c("medal_type", "country_code_scaled", "discipline_code_scaled", "event_scaled")]
```

```{r}
medals_knn_test <- shuffled_medals_knn[(split+1):nrow(shuffled_medals_knn),c("medal_type", "country_code_scaled", "discipline_code_scaled", "event_scaled")]
```

```{r}
medal_types.train <- medals_knn_train$medal_type
```

```{r}
p <- knn(train = medals_knn_train[-1], test = medals_knn_test[-1], cl = medal_types.train, k = 3)
```

### Confusion matrix for kNN model

```{r}
table(medals_knn_test$medal_type, p, dnn=c("Actual","Predicted"))
```


### Compute accuracy of kNN model

```{r}
knn.accuracy <- mean(p == medals_knn_test$medal_type)
knn.accuracy
```


### Calculate misclassification rate of kNN model

```{r}
num.incorrect <- sum(p != medals_knn_test$medal_type)
num.correct <- sum(p == medals_knn_test$medal_type)
```

```{r}
misclassification.rate <- num.incorrect / num.correct
```

```{r}
misclassification.rate
```


## Decision Tree Model

- Create training and test data sets in which 80% of the data will be for training.  The remainig 20% will be for the test set.

- Create Decision Tree model for a binary classification of Gold medals won


```{r}
shuffled_medals_dtree <- shuffle_data(medals_decision_tree)
```

```{r}
split.dtree <- round(nrow(shuffled_medals_dtree)*0.80)
```

```{r}
medals_dtree_train <- shuffled_medals_dtree[1:split.dtree, 
                                            c("gold", "medal_type", "country_code", "discipline_code", "event")]
```

```{r}
medals_dtree_test <- shuffled_medals_dtree[(split.dtree+1):nrow(shuffled_medals_dtree), 
                                           c("gold", "medal_type", "country_code", "discipline_code", "event")]
```

### Remove country codes in test set that do not exist in the training set


```{r}
distinct_country_code.train <- unique(medals_dtree_train$country_code)
distinct_country_code.test <- unique(medals_dtree_test$country_code)
```

```{r}
country_code.notin_train <- setdiff(distinct_country_code.test, distinct_country_code.train)
```

```{r}
country_code.notin_train
```

```{r}
medals_dtree_test <- medals_dtree_test %>% 
  filter(!(country_code %in% country_code.notin_train)) %>% 
  filter(!(event %in% "Men's Triple Jump"))
```

```{r}
gold_model <- rpart(gold ~ country_code + discipline_code + event, 
                    data = medals_dtree_train, method = "class", control = rpart.control(cp = 0))
```

```{r summary of gold_model}
str(gold_model)
```
```{r}
plot(gold_model)
```

This appears to be deep decision tree with the maxdepth value of 30.  The number of splits is 205 since there are many combinations of countries, disciplines, and events in the trainig set.  


### Perform In-sample prediction for Decision Tree


```{r}
medals_dtree_train$pred <- predict(gold_model, type = "class")
```

### In-sample Confusion matrix for Decision Tree model

```{r}
table(medals_dtree_train$gold, medals_dtree_train$pred, dnn=c("Actual","Predicted"))
```

### In-Sample Accuracy score for Decision Tree model

```{r}
dtreee.insample.accuracy <- mean(medals_dtree_train$gold == medals_dtree_train$pred)
dtreee.insample.accuracy
```

### In-Sample misclassification rate for Decision Tree model

```{r}
num.incorrect.dtree.insample <- sum(medals_dtree_train$gold != medals_dtree_train$pred)
num.correct.dtree.insample <- sum(medals_dtree_train$gold == medals_dtree_train$pred)
```

```{r}
misclassification.rate.dtree.insample <- num.incorrect.dtree.insample / num.correct.dtree.insample
```

```{r}
misclassification.rate.dtree.insample
```

The accuracy is very high at 95% and the misclassification rate is below 5%. The decision tree is also deep in that it has a maximum depth of 30 levels. Both factors may indicate overfitting. The decision tree will be applied to an out-of-sample test set to determine if the model can generalize to unseen data.


### Perform Out-Of-Sample prediction for Decision Tree


```{r}
medals_dtree_test$pred <- predict(gold_model, medals_dtree_test, type = "class")
```

### Out-Of-Sample Confusion matrix for Decision Tree model

```{r}
table(medals_dtree_test$gold, medals_dtree_test$pred, dnn=c("Actual","Predicted"))
```

### Out-Of-Sample Accuracy score for Decision Tree model

```{r}
dtreee.out_sample.accuracy <- mean(medals_dtree_test$gold == medals_dtree_test$pred)
dtreee.out_sample.accuracy
```

### Out-Of-Sample misclassification rate for Decision Tree model

```{r}
num.incorrect.dtree.outsample <- sum(medals_dtree_test$gold != medals_dtree_test$pred)
num.correct.dtree.outsample <- sum(medals_dtree_test$gold == medals_dtree_test$pred)
```

```{r}
misclassification.rate.dtree.outsample <- num.incorrect.dtree.outsample / num.correct.dtree.outsample
```

```{r}
misclassification.rate.dtree.outsample
```

There appears to be overfitting of the training data for the decision tree model.  The accuracy for the test set was 79% which is a significant drop from the In-sample 95% accuracy score.


## Cross-Validation

### kNN Cross-Validation


```{r}
cv_knn_model <- train(medal_type ~ ., data = medals_knn_train, 
                      method = "knn",
                      tuneGrid=data.frame(k=3),
                      trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE))
```

```{r}
cv_knn_model
```
```{r cv_knn_accuracy}
cv_knn_accuracy <- cv_knn_model$results$Accuracy
```


### Decision tree Cross-Validation


```{r}
cv_dtree_model <- train(gold ~ country_code + discipline_code + event, 
                        data = medals_dtree_train, 
                        method = "rpart",
                        trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE))
```

```{r}
cv_dtree_model
```

```{r cv_dtree_accuracy}
cv_dtree_accuracy <- cv_dtree_model$results$Accuracy[1]
```


```{r accuracy comparison table}
model <- c("knn", "decision tree")
accuracy <- c(knn.accuracy, dtreee.out_sample.accuracy)
cv_accuracy <- c(cv_knn_accuracy, cv_dtree_accuracy)

df <- data.frame(model, accuracy, cv_accuracy)
df
```


## Findings

**Which classifier was more accurate?** 

The Decision Tree was more accurate than the k-Nearest Neighbors with k = 3



**Was it more accurate with or without cross-validation?**

The Decision without cross-validation produced an accuracy of 79% which was higher than the kNN accuracy



**What statistic did you use to compare their performance?**

The accuracy statistic was used to compare the performance of the kNN and Decision Tree models 



## References

k-Nearest Neighbor: An Introductory Example.
https://quantdev.ssri.psu.edu/sites/qdev/files/kNN_tutorial.html


Classification and Regression Tree (CART).
https://homepages.uc.edu/~lis6/Teaching/ML19Spring/Lab/lab8_tree.html#classification-tree-credit-card-default-data


