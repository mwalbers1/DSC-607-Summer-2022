---
title: "DSC-607 Data Mining: Clustering Models"
author: "Michael Albers"
date: '2022-06-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Overview

- Discuss the theoretical background of cluster analyses (in general)

- Perform cluster analysis using two different methods and interpret and discuss the results.

- How do the results of the two methods differ? Why?

- What statistic did you use to compare the performance of your two clustering algorithms?



## Data Set

This data set contains information on New York City air quality surveillance data. The agency is Department of Health and Mental Hygiene (DOHMH). The data set was downloaded from https://data.cityofnewyork.us/Environment/Air-Quality/c3uy-2p5r.

Air pollution is one of the most important environmental threats to urban populations and while all people are exposed, pollutant emissions, levels of exposure, and population vulnerability vary across neighborhoods. Exposures to common air pollutants have been linked to respiratory and cardiovascular diseases, cancers, and premature deaths. These indicators provide a perspective across time and NYC geographies to better characterize air quality and health in NYC.




## Clustering Background

Partitional clustering divides data points into a finite number of non-overlapping partitions.  Hierarchical clustering places data points into a set of nested clusters, that are organized into a hierarchical tree.  Exclusive clustering is when the points belong to only one cluster.  In non-exclusive clustering, points can belong to multiple clusters.  Fuzzy clustering means that every point belongs to every cluster with some weight between zero and one.




```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(skimr)
library(janitor)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(cluster)
library(dbscan)
```


```{r}
aq <- read.csv('data/Air_Quality.csv')
```

```{r}
glimpse(aq)
```

```{r}
aq_clean <- aq %>% clean_names()
```

```{r}
colnames(aq_clean)
```

```{r}
summary(aq_clean)
```

```{r}
categ_cols <- aq_clean %>%
  tabyl(name, measure) %>%
  adorn_totals("col")
```

```{r}
category_df <- as.data.frame(categ_cols)
```

```{r}
category_df
```



```{r}
measure_col_stats <- aq_clean %>%
  tabyl(measure) %>%
  adorn_totals("col")
```

```{r}
measure_col_stats_df <- as.data.frame(measure_col_stats)
```

```{r}
measure_col_stats_df
```

## Feature Selection

The clustering analysis will be performed on the following five features:

1. Air Toxics Concentrations- Average Benzene Concentrations		
2. Air Toxics Concentrations- Average Formaldehyde Concentrations		
3. Boiler Emissions- Total NOx Emissions		
4. Boiler Emissions- Total PM2.5 Emissions		
5. Boiler Emissions- Total SO2 Emissions


## K-Means Clustering

### Data Preparation for K-Means model

Create a wide data frame with each feature as its own column

```{r}
air_quality_names <- c('Air Toxics Concentrations- Average Benzene Concentrations', 
                       'Air Toxics Concentrations- Average Formaldehyde Concentrations',
                       'Boiler Emissions- Total NOx Emissions',
                       'Boiler Emissions- Total PM2.5 Emissions',
                       'Boiler Emissions- Total SO2 Emissions')

```

```{r}
air_quality.filtered <- aq_clean %>% 
  filter(name %in% air_quality_names) %>% 
  subset(select = c(indicator_id, name, data_value))
```

```{r}
atc_avg_benzene_values <- air_quality.filtered %>% 
    filter(indicator_id == 646) %>% 
    select(atc_avg_benzene=data_value)

```

```{r}
atc_avg_formaldehyde_values <- air_quality.filtered %>% 
    filter(indicator_id == 647) %>% 
    select(atc_avg_formaldehyde=data_value)
```

```{r}
boiler_nox_values <- air_quality.filtered %>% 
    filter(indicator_id == 642) %>% 
    select(boiler_nox=data_value)
```

```{r}
boiler_pm2.5_values <- air_quality.filtered %>% 
    filter(indicator_id == 641) %>% 
    select(boiler_pm2.5=data_value)
```

```{r}
boiler_so2_values <- air_quality.filtered %>% 
    filter(indicator_id == 640) %>% 
    select(boiler_so2=data_value)
```

```{r}
df.t1 <- data.frame(atc_avg_benzene = atc_avg_benzene_values,
                    atc_avg_formaldehyde = atc_avg_formaldehyde_values)

df.t2 <- data.frame(boiler_nox = boiler_nox_values,
                    boiler_pm2.5 = boiler_pm2.5_values,
                    boiler_so2 = boiler_so2_values)

                    
```

```{r}
air_quality_wide.df <- cbind(df.t1[1:96,], df.t2)
```

### Scale data

The five features in the air_quality_wide.df data frame are in different scales.  The data frame will be scaled to prevent any column(s) from having too much influence on the K-Means algorithm.


```{r}
air_quality_scaled <- scale(air_quality_wide.df)
```

```{r}
class(air_quality_scaled)
```

## Implement K-Means Algorithm

### Elbow Method to find optimal number of clusters

```{r}
set.seed(6)
wcss <- vector()

for (i in 1:10) {
  wcss[i] <- sum(kmeans(air_quality_scaled, i)$withinss)
}

```

```{r}
plot(1:10,
   wcss,
   type = 'b',
   main = paste('The Elbow Method'),
   xlab = 'Number of clusters',
   ylab = 'WCSS')
```

The optimal number of clusters appears to be five from the scree plot

```{r}
set.seed(29)
kmeans.aq <- kmeans(x = air_quality_scaled, centers = 5)
y_kmeans = kmeans.aq$cluster
```

### Visualize K-Means clusters



```{r}
clusplot(air_quality_wide.df[1:2],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 1,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Air Quality Threats'),
         xlab = 'Average Benzene',
         ylab = 'Average Formaldehyde')
```

Visualization of clusters with the "Air Toxics Concentrations - Average Benzene Concentrations" and "Air Toxics Concentrations - Average Formaldehyde Concentrations" features



```{r}
clusplot(air_quality_wide.df[3:4],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 1,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Air Quality Threats'),
         xlab = 'Boiler Emissions - Total Nox',
         ylab = 'Boiler Emissions - Total PM2.5')
```

Visualization of clusters with the "Boiler Emissions- Total NOx Emissions" and "Boiler Emissions- Total PM2.5 Emissions" features


### Calculate proportion of WSS to the Total Sum of Squares

```{r}
aq.wss.proportion <- kmeans.aq$tot.withinss / kmeans.aq$totss

```

```{r}
print(aq.wss.proportion)
```

Both cluster plots are showing outliers and the clusters are overlapping one another. The Total WSS (Within Cluster Sum of Squared distances) accounts for 19% of the total Sum of Squares.  There is one cluster in which its points have high density around its centroid while the remaining clusters have low density around their respective centroids.


## DBSCAN Clustering Algorithm

Because there are high density points in the data set, a density-based clustering model will be created


### Plot k-Nearest Neighbor Distance

Use this plot to find a suitable value for the eps parameter for DBSCAN clustering model

```{r}
kNNdistplot(air_quality_scaled, k=4)
```

The kNN distance plot increases at 0.25


```{r}
dbscan.hyperparams <- data.frame(eps = c(0.25, 0.3), minPts = c(2, 4))
row.names(dbscan.hyperparams) <- c('Iteration-1', 'Iteration-2')
```



## Implement DBSCAN Clustering

### Hyperparameters

Run two iterations of DBSCAN with the following hyperparameters:

```{r}
print(dbscan.hyperparams)
```

#### Iteration - 1

```{r}
set.seed(4432)
y_dbscan <- dbscan(air_quality_scaled, eps = 0.25, minPts = 2)
```

```{r}
table(y_dbscan$cluster)
```


```{r}
plot(air_quality_wide.df[3:4], main = "DBSCAN plot - 1", col=y_dbscan$cluster)
```

The first iteration of DBSCAN produced seven clusters. Four clusters have less than ten points in each cluster.  There are two outliers which appear in its own cluster.


#### Iteration - 2

```{r}
set.seed(4432)
y_dbscan.two <- dbscan(air_quality_scaled, eps = 0.3, minPts = 4)
```

```{r}
table(y_dbscan.two$cluster)
```

```{r}
plot(air_quality_wide.df[3:4], main = "DBSCAN plot - 2", col=y_dbscan.two$cluster)
```

The second iteration resulted in four clusters in which the two outliers were excluded from the first iteration.  Two of the four clusters have less than ten points in each cluster.



## Clustering Evaluation


### Evaluation of K-Means Clustering model

The Silhouette ratio will be calculated for the K-Means model


```{r}
dist_matrix <- dist(air_quality_scaled)
```

```{r}
kmeans.aq.silhouette <- silhouette(y_kmeans, dist_matrix)
```

```{r}
summary(kmeans.aq.silhouette)
```


### Evaluation of DBSCAN Clustering model

Calculate the Silhouette ratio for the second iteration of the DBSCAN model


```{r}
dbscan.silhouette <- silhouette(y_dbscan.two$cluster, dist_matrix)
```

```{r}
summary(dbscan.silhouette)
```


## Results

- The best Silhouette ratio from the DBSCAN model was 0.66 (for cluster 2). The highest Silhouette ratio from the K-Means model was 0.59 (for cluster 3)

- The DBSCAN model did a better job of clustering by eliminating outliers (noise).

- The DBSCAN model produced four clusters as compared to the five clusters produced by the K-Means model



## References


Hadelin de Ponteves. Machine Learning A-Z: Hands-On Python & R in Data Science. SuperDataScience. Retrieved from https://www.udemy.com/share/101Wci3@NBBFQDfFLzRGxoO4-Ik4lxoYKhdn8weeCIJWoWEjsR1VFIVJ-QLuyIHt7D27dxUnkA==/


kNNdistplot: Plot the k-Nearest Neighbor Distance.
https://www.rdocumentation.org/packages/dbscan/versions/0.9-0/topics/kNNdistplot






